# Метрические алгоритмы классификации
Метрические методы обучения - методы, основанные на анализе сходства объектов. Рассмотрим несколько разновидностей таких методов.
Алгоритм 1NN и K ближайших соседей (KNN)

## Метод ближайшего сосседа (1NN):
Алгоритм ближайшего соседа крайне прост: подбирается метрика. В данной работе это декартово расстояние между векторами. Обучающая выборка сортируется в порядке увеличения расстояния от классифицируемого элемента. Элемент относят к тому классу к которому принадлежит ближайший (первый в отсортированной выборке) элемент.
### Достоинства данного алгоритма:
+ простота;
+ объяснимость выводов. 
### Недостатки: 
+ неустойчивость к погрешностям; 
+ низкое качество классификации; 
+ отсутствие гибкости, то есть при 
построении алгоритма не учитыввается 
достаточно большое количество информации.
Некоторым решением может являться алгоритм k ближайших соседей.

## Метод K ближайших соседей (КNN):
В случае использования метода для классификации объект присваивается тому классу, который является наиболее распространённым среди k соседей данного элемента, классы которых уже известны. Работает данный алгоритм следующим образом: пусть дан классифицируемый объект *z* и обучающая выборка ![](http://latex.codecogs.com/gif.latex?%24X%5El%24). Требуется определить класс объекта *z* на основе данных из обучающей выборки. Для этого:
1. Вся выборка ![](http://latex.codecogs.com/gif.latex?%24X%5El%24) сортируется по возрастанию расстояния от объекта *z* до каждого объекта выборки.
2. Проверяются классы *k* ближайших соседей объекта *z*. Класс, встречаемый наиболее часто, присваивается объекту *z*.  

Вход алгоритма - классифицируемый объект, обучающая выборка и параметр *k* - число рассматриваемых ближайших соседей.
Выход алгоритма - класс классифицируемого объекта.

При *k = 1* алгоритм превращается в *1NN*, то есть объекту *z* присваивается класс его первого ближайшего соседа, все остальные объекты выборки не учитываются.

Оптимальное значение *k* подбирается по Критерию Скользящего Контроля (*LOO*). Суть критерия: пусть дана обучающая выборка ![xl](http://latex.codecogs.com/gif.latex?%24X%5El%24). Требуется определить оптимальное значение параметра *k* для данной выборки. Для этого:
1. Из выборки удаляется *i*-й объект ![](http://latex.codecogs.com/gif.latex?x_%7Bi%7D).
2. Выбранный алгоритм классификации запускается для ![x_i](http://latex.codecogs.com/gif.latex?x_%7Bi%7D) и оставшейся выборки. По окончании работы алгоритма полученный класс объекта ![](http://latex.codecogs.com/gif.latex?x_%7Bi%7D) сравнивается с его реальным классом. При их несовпадении сумма ошибки увеличивается на 1.
3. Шаги 1 и 2 повторяются для каждого объекта выборки при фиксированном *k*. По окончании работы алгоритма полученная сумма ошибки *sum* делится на размер выборки *l*: ![sum=sum/l](http://latex.codecogs.com/gif.latex?sum%3D%20%5Cfrac%7Bsum%7D%7Bl%7D) .  Потом значение *k* меняется, и алгоритм повторяется для нового значения. *k* с наименьшим значением суммы ошибки будет оптимальным.
#### Реализация
При реализации алгоритма, в качестве обучающей выборки использовалась выборка ирисов Фишера. В качестве признаков объектов использовались значения длины и ширины лепестка. Значение *k* подбиралось по *LOO*.
Достоинства алгоритма:
1. Простота реализации
2. Хорошее качество, при правильно подобранной метрике и параметре *k*

Недостатки алгоритма:
1. Необходимость хранить выборку целиком
2. Малый набор параметров
3. Качество классификации сильно зависит от выбранной метрики
