# Байесовские алгоритмы классификации
В настоящее время статистические методы широко применяются для классификации текстов по признакам авторского, жанрового, гендерного и других  стилей. Байесовская  теория  принятия  решений  составляет  основу статистического  подхода  к  задаче  классификации  объектов.Этот  подходоснован на предположении, что задача выбора решения сформулирована в терминах  теории  вероятностей  и  известны  все  представляющие  интерес вероятностные  величины. В  основе  байесовской  классификации  лежит правило Байеса.
### Нормальный дискриминантный анализ
Это метод классификации, основанный на параметрическом подходе, специальный случай, при котором плотности распределения всех классов полагаются многомерными нормальными.  
![](http://latex.codecogs.com/gif.latex?N%28x%2C%20%5Cmu%2C%20%5CSigma%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%28%282%5Cpi%29%5E%7Bn%7D%7C%5CSigma%7C%29%7De%5E%7B-%5Cfrac%7B1%7D%7B2%7D%28%28x-%5Cmu%29%5CSigma%5E%7B-1%7D%28x-%5Cmu%29%5E%7BT%7D%29%7D), где  
![](http://latex.codecogs.com/gif.latex?x%20%5Cin%20R%5E%7Bn%7D) - объект, состоящий из *n* признаков  
![](http://latex.codecogs.com/gif.latex?%5Cmu%20%5Cin%20R%5E%7Bn%7D) - мат. ожидание каждого признака  
![](http://latex.codecogs.com/gif.latex?%5CSigma%20%5Cin%20R%5E%7Bn%5Ctimes%20n%7D) - матрица ковариации признаков. Симметричная, невырожденная, положительно определённая.  
#### Линии уровня нормального распределения
1. Если признаки некоррелированы, т. е. матрица ковариации диагональна, то линии уровня имеют форму эллипсоидов, параллельных осям координат, вытянутых относительно признака, значение для которого в матрице выше.
2. Если признаки коррелированы, то есть матрица ковариации не диагональна, то линии уровня имеют форму эллипсоидов, наклонённых относительно осей координат.
Если предположить, что все признаки объектов выборки сформированы независимо, то значение функции плотности ![](http://latex.codecogs.com/gif.latex?p_%7By%7D%28x%29) для класса *y* для объекта *x* можно представить в виде ![](http://latex.codecogs.com/gif.latex?p_%7By%7D%28x%29%3Dp_%7By%7D%28%5Cxi_%7B1%7D%29*%5Cdots*p_%7By%7D%28%5Cxi_%7Bn%7D%29), где ![](http://latex.codecogs.com/gif.latex?%5Cxi_%7Bi%7D) - *i*-й признак объекта *x*. Это упрощает задачу, так как оценивать несколько одномерных плотностей легче, чем одну многомерную.  
Однако, на практике такая ситуация встречается редко, поэтому алгоритм получил название "Наивный байесовский классификатор". Обычно он используется, как эталон при сравнении различных алгоритмов классификации.  
Решающее правило принимает вид:  
![](http://latex.codecogs.com/gif.latex?a%28x%29%3Darg%20%5Cmax_%7By%5Cin%20Y%7D%28%5Cln%28%5Clambda_%7By%7DP_y%29&plus;%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Cln%28p_%7Byj%7D%28%5Cxi_j%29%29%29)  
### Подстановочный (Plug-in) алгоритм
Алгоритм относится к нормальному дискриминантному анализу и применяется для многомерных (в данном случае - для многомерного нормального) распределений.  
Решающее правило имеет вид:
![](http://latex.codecogs.com/gif.latex?a%28x%29%3Darg%5Cmax_%7By%5Cin%20Y%7D%5Clambda_y%5Chat%20P_y%5Chat%20p_y%28x%29), где ![](http://latex.codecogs.com/gif.latex?%5Chat%20P_y%2C%5Chat%20p_y%28x%29) - восстановленные вероятность и плотность распределения класса соответственно.  
![](http://latex.codecogs.com/gif.latex?%5Chat%20%5Cmu%28x%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Em_%7Bi%3D1%7Dx_i)  
![](http://latex.codecogs.com/gif.latex?%5Chat%20p_y%28x%29%20%3D%20N%28x%2C%20%5Cmu%2C%20%5CSigma%29%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%28%282%5Cpi%29%5En%7C%5CSigma%7C%29%7D%5Cexp%28-%5Cfrac%7B1%7D%7B2%7D%28x-%5Cmu%29%5CSigma%5E%7B-1%7D%28x-%5Cmu%29%5ET%29)  
где ![](http://latex.codecogs.com/gif.latex?%5CSigma) - восстановленная матрица ковариации.
Матрица ковариации вычисляется по формуле:  
![](http://latex.codecogs.com/gif.latex?%5CSigma%28x%2C%20%5Cmu%29%20%3D%20%5Cfrac%7B1%7D%7Bm-1%7D%5Csum%5Em_%7Bi%3D1%7D%28x_i-%5Cmu%29%28x_i-%5Cmu%29%5ET)  
